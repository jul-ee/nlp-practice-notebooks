{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 텍스트 데이터 다루기"],"metadata":{"id":"6T3GTyXJ9RrE"}},{"cell_type":"markdown","source":["## 1. 전처리: 자연어 노이즈 제거"],"metadata":{"id":"bZJGxl0s9TuU"}},{"cell_type":"markdown","source":["### 1.1 노이즈 유형: 문장부호"],"metadata":{"id":"Z91ZSPzy_AFy"}},{"cell_type":"markdown","source":["- 문장부호(구두점) 앞뒤에 공백 추가"],"metadata":{"id":"41qT3M7P_Hec"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nsL0l314sT1A","outputId":"59a1c4cb-b709-4655-8951-258479598b2e","executionInfo":{"status":"ok","timestamp":1749697830014,"user_tz":-540,"elapsed":18,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Hi ,  my name is john . \n"]}],"source":["\"\"\"\n","str.replace(old, new)\n","- 문자열에서 old 값을 찾아 new 값으로 모두 바꾸는 함수\n","- 새로운 문자열을 반환하며, 원본 문자열은 변하지 않음\n","\"\"\"\n","def pad_punctuation(sentence, punc):\n","    # 주어진 문장부호 리스트(punc)에 있는 각 기호에 대해 반복\n","    for p in punc:\n","        # 해당 문장부호 앞뒤에 공백을 추가하여 문장 내 모든 문장부호 변환\n","        sentence = sentence.replace(p, \" \" + p + \" \")\n","    # 공백이 추가된 문장 반환\n","    return sentence\n","\n","sentence = \"Hi, my name is john.\"\n","\n","# 문장부호 리스트\n","print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"]},{"cell_type":"markdown","source":["<br>\n","\n","### 1.2 노이즈 유형: 대소문자"],"metadata":{"id":"t5CZui2c_0V1"}},{"cell_type":"code","source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.lower())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjseDN2_sd4X","outputId":"8c91e0c6-670b-438b-ef8e-dfa311089193","executionInfo":{"status":"ok","timestamp":1749697833764,"user_tz":-540,"elapsed":17,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["first, open the first chapter.\n"]}]},{"cell_type":"code","source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.upper())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9ujrrxjse_u","outputId":"9da5cda7-8c87-49e3-a7bb-c95bd300e515","executionInfo":{"status":"ok","timestamp":1749697835434,"user_tz":-540,"elapsed":5,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["FIRST, OPEN THE FIRST CHAPTER.\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### 1.3 노이즈 유형: 특수문자"],"metadata":{"id":"KNgnF6ir_-Ak"}},{"cell_type":"markdown","source":["정규표현식(regular expression)\n","- 문장에서 특정 문자를 제외한 나머지를 공백으로 치환"],"metadata":{"id":"7oZJ9dCQAWBC"}},{"cell_type":"code","source":["import re\n","\n","sentence = \"He is a ten-year-old boy.\"\n","# 알파벳 소문자/대문자와 .,?!를 제외한 나머지 문자를 공백으로 치환\n","# → 공백, 하이픈(-), 숫자, 특수기호 등\n","sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n","\n","print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMsV5uMLsiNk","outputId":"2aa35621-56d8-4802-d084-a8beba041d9a","executionInfo":{"status":"ok","timestamp":1749697838019,"user_tz":-540,"elapsed":6,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["He is a ten year old boy.\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### 1.4: 텍스트 정제 전처리 함수"],"metadata":{"id":"X18iALxXBXgg"}},{"cell_type":"markdown","source":["결과 해석\n","- “w-a-t-e-r” → \"w a t e r\" (하이픈 제거)\n","- 대소문자 → 소문자로 통일됨\n","- 마침표와 쉼표 → 앞뒤로 공백 삽입됨\n","- 특수문자 —는 제거됨\n","- 줄바꿈은 그대로 유지됨\n"],"metadata":{"id":"1UKxURlOCd2x"}},{"cell_type":"code","source":["# From The Project Gutenberg\n","# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n","\n","corpus = \\\n","\"\"\"\n","In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk.\n","But my teacher had been with me several weeks before I understood that everything has a name.\n","One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered.\n","Some one was drawing water and my teacher placed my hand under the spout.\n","As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly.\n","I stood still, my whole attention fixed upon the motions of her fingers.\n","Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me.\n","I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand.\n","That living word awakened my soul, gave it light, hope, joy, set it free!\n","There were barriers still, it is true, but barriers that could in time be swept away.\n","\"\"\"\n","\n","def cleaning_text(text, punc, regex):\n","    # (1) 문장부호 공백추가\n","    for p in punc:\n","        text = text.replace(p, \" \" + p + \" \")\n","\n","    # (3) 특수문자 제거 + (2) 소문자로 변환\n","    text = re.sub(regex, \" \", text).lower()\n","\n","    return text\n","\n","print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSKgc9gysji0","outputId":"bac33cce-42d7-4260-8c00-2fab2984b74e","executionInfo":{"status":"ok","timestamp":1749697840752,"user_tz":-540,"elapsed":18,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk . \n","but my teacher had been with me several weeks before i understood that everything has a name . \n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered . \n","some one was drawing water and my teacher placed my hand under the spout . \n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly . \n","i stood still ,  my whole attention fixed upon the motions of her fingers . \n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me . \n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand . \n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free ! \n","there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n","\n"]}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","\n","## 2. 토큰화\n"],"metadata":{"id":"N0IaqVYUCVsf"}},{"cell_type":"markdown","source":["### 2.1 공백 기반 토큰화"],"metadata":{"id":"EHo1rNWRGb9B"}},{"cell_type":"markdown","source":["문장을 공백으로 나누어 토큰화\n","- 단어 단위로 토큰화를 수행하며, 간단하고 빠른 토큰화 방법\n","- 하지만 단어 내의 구두점, 대소문자, 축약어 등을 처리하지 못하며\n","- 영어에서는 약어나 이니셜 등이 문제가 됨"],"metadata":{"id":"gPKJOEBmNt-9"}},{"cell_type":"code","source":["corpus = \\\n","\"\"\"\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .\n","but my teacher had been with me several weeks before i understood that everything has a name .\n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .\n","some one was drawing water and my teacher placed my hand under the spout .\n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .\n","i stood still ,  my whole attention fixed upon the motions of her fingers .\n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .\n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .\n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !\n","there were barriers still ,  it is true ,  but barriers that could in time be swept away .\n","\"\"\"\n","\n","tokens = corpus.split()\n","\n","print(\"문장이 포함하는 Tokens:\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Edk3kQYOsnKC","outputId":"5fecec66-cce0-4406-d87e-dc39fac2b6ea","executionInfo":{"status":"ok","timestamp":1749697844436,"user_tz":-540,"elapsed":26,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### 2.2 형태소 기반 토큰화"],"metadata":{"id":"blFFGrn1GukJ"}},{"cell_type":"markdown","source":["단어의 의미를 고려하여 토큰화\n","- 형태소: 단어를 구성하는 가장 작은 의미 단위\n","- 단어 내의 구성 요소인 접두사, 접미사, 어근 등을 고려하여 토큰화\n","- 단어의 형태와 문법적인 정보를 보존할 수 있으며, 정보 검색이나 언어 학습에서 효과적"],"metadata":{"id":"jpZIvUhaN3DM"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IV2cXPgtRyc","outputId":"ad6de000-6b4a-49c3-d27e-e3806cbe1fd7","executionInfo":{"status":"ok","timestamp":1749697854793,"user_tz":-540,"elapsed":8042,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Hannanum,Kkma,Komoran,Okt  # Mecab 제외 (설치 이슈)"],"metadata":{"id":"lZZGIvsMtTho","executionInfo":{"status":"ok","timestamp":1749697857376,"user_tz":-540,"elapsed":115,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer_list = [Hannanum(), Kkma(), Komoran(), Okt()] # Mecab() 제외\n","\n","kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n","\n","for tokenizer in tokenizer_list:\n","    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCXqEMRFsn9f","outputId":"d6bee3f9-f164-4bda-a7c6-c49352837523","executionInfo":{"status":"ok","timestamp":1749697905360,"user_tz":-540,"elapsed":42402,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[Hannanum] \n","[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n","[Kkma] \n","[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n","[Komoran] \n","[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n","[Okt] \n","[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"]}]},{"cell_type":"markdown","source":["```\n","[Mecab]\n","[('코로나', 'NNP'), ('바이러스', 'NNG'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNBC'), ('12', 'SN'), ('월', 'NNBC'), ('중국', 'NNP'), ('우한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('한', 'XSV+ETM'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKB'), ('확산', 'NNG'), ('된', 'XSV+ETM'), (',', 'SC'), ('새로운', 'VA+ETM'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n","```"],"metadata":{"id":"CXG5iT_jIqea"}},{"cell_type":"markdown","source":["공백 기반이나 형태소 기반의 토큰화 기법들은 모두 의미를 가지는 단위로 토큰을 생성한다. 이 기법의 경우, 데이터에 포함되는 모든 단어를 처리할 수는 없기 때문에 자주 등장한 상위 N개의 단어만 사용하고 나머지는 <unk>같은 특수한 토큰(Unknown Token)으로 치환해버린다. 자원과 데이터가 한정되어 있기 때문에 이 또한 불가피한 손실이다.\n","\n","하지만 이것은 종종 OOV(Out-Of-Vocabulary) 문제를 야기한다.\n","- 모델이 학습할 때는 본 적이 없는 단어나 문장이 테스트 데이터에 등장하여 발생하는 문제\n","- 이러한 단어나 문장은 모델이 인식하지 못하고, 예측 오류를 일으키게 됨\n","- 이를 해결하고자  Wordpiece Model이 등장\n","\n","Wordpiece Model(WPM)\n","- pre+view와 pre+dict로 보는 것처럼\n","- 한 단어를 여러 개의 Subword의 집합으로 보는 방법\n","\n"],"metadata":{"id":"l3wV9IgWJbWl"}},{"cell_type":"markdown","source":["<br>\n","\n","### 2.3 Byte Pair Encoding(BPE)\n","\n","단어를 기본 단위인 문자(character) 단위로 분해한 후, 가장 자주 등장하는 문자의 쌍(pair)을 하나의 문자로 대체하는 방식으로 동작\n","\n","- 모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치면\n","- 접두어나 접미어의 의미를 캐치할 수 있고\n","- 처음 등장하는 단어는 문자(알파벳)들의 조합으로 나타내어\n","- OOV 문제를 완전히 해결할 수 있다는 것\n","\n","이를 통해 기존에 없던 단어가 등장하더라도, 해당 단어를 구성하는 문자들의 조합이 기존에 등장했던 문자 쌍과 일치하는 경우, 기존에 등장했던 토큰들의 조합으로 변환하여 OOV 문제를 해결할 수 있다.\n","\n","> BPE는 OOV 문제를 해결하고, 텍스트 데이터를 더 효율적으로 인코딩하여 데이터의 압축을 향상시키는 효과를 가지고 있다.\n","\n","<br>\n","\n","[참고] 정규표현식 Lookaround\n","\n","| 문법         | 이름                  | 의미 (의역)         | 방향       |\n","| ---------- | ------------------- | --------------- | -------- |\n","| `(?=...)`  | Lookahead           | 뒤에 이게 **있어야 함** | 앞에서 뒤 확인 |\n","| `(?!...)`  | Negative Lookahead  | 뒤에 이게 **없어야 함** | 앞에서 뒤 확인 |\n","| `(?<=...)` | Lookbehind          | 앞에 이게 **있어야 함** | 뒤에서 앞 확인 |\n","| `(?<!...)` | Negative Lookbehind | 앞에 이게 **없어야 함** | 뒤에서 앞 확인 |\n"],"metadata":{"id":"UNuPnhuLHxOo"}},{"cell_type":"code","source":["\"\"\"\n","Byte Pair Encoding(BPE) 알고리즘의 간단한 예제 구현\n",": 단어들을 점차적으로 더 큰 단위의 subword(부분단어) 토큰으로 병합하는 과정 반복\n","\n","    1. 단어(공백으로 분리된 문자들의 나열)와 빈도수가 주어졌을 때\n","    2. 가장 자주 등장하는 문자 쌍을 찾아 하나의 토큰처럼 취급하고\n","    3. 이를 여러 단계(num_merges)에 걸쳐 반복하여\n","    4. 서브워드 단위로 어휘를 축소(압축)하는 방식\n","    → NLP에서 subword tokenization의 핵심 아이디어\n","\"\"\"\n","\n","import re, collections\n","\n","# 초기 단어 사전\n","#  key: 공백으로 분리된 문자들의 나열\n","#  value: 해당 단어가 등장한 횟수(빈도수)\n","vocab = {\n","    'l o w '      : 5,\n","    'l o w e r '  : 2,\n","    'n e w e s t ': 6,\n","    'w i d e s t ': 3\n","}\n","\n","num_merges = 5  # 병합 수행 횟수\n","\n","\n","# 문자쌍 빈도수 계산 함수\n","def get_stats(vocab):\n","    \"\"\"\n","    현재 단어 사전에서 등장하는 문자 쌍(2-gram)의 빈도수 계산\n","\n","    Parameters\n","    ----------\n","    vocab : dict\n","        key: 공백 단위 문자로 이루어진 단어(str)\n","        value: 해당 단어의 빈도수(int)\n","\n","    Returns\n","    -------\n","    dict\n","        key: 문자쌍(tuple of str)\n","        valye: 총 등장 빈도수(int)\n","        {\n","          ('e', 's'): 6,\n","           ...\n","        }\n","    \"\"\"\n","    pairs = collections.defaultdict(int)\n","\n","    for word, freq in vocab.items():\n","        # 공백 기준 문자 리스트 생성\n","        symbols = word.split()  # 'l o w ' → ['l', 'o', 'w']\n","\n","        for i in range(len(symbols) - 1):\n","            pairs[symbols[i], symbols[i + 1]] += freq  # 문자쌍 빈도수 누적\n","\n","    return pairs\n","\n","\n","# 문자쌍 병합 함수\n","def merge_vocab(pair, v_in):\n","    \"\"\"\n","    가장 자주 등장한 문자쌍을 하나의 토큰처럼 병합하여 vocab 업데이트\n","\n","    Parameters\n","    ----------\n","    pair : tuple of str\n","        병합할 문자쌍  e.g., ('e', 's')\n","    v_in : dict\n","        현재 vocab 사전\n","\n","    Returns\n","    -------\n","    v_out : dict\n","        문자쌍 병합 후 업데이트된 vocab\n","    merge_token : str\n","        병합된 새 토큰  e.g., 'es'\n","    \"\"\"\n","    v_out = {}\n","\n","    # ('e', 's') → 'e s'\n","    # 정규표현식 특수문자 대응을 위해 escape 처리: 'e s' → 'e\\\\ s'\n","    bigram = re.escape(' '.join(pair))\n","\n","    # 정규표현식 객체 생성\n","    # (?<!\\S): 바로 앞이 '공백' 또는 줄의 시작 (단어 경계 앞)\n","    #   → (?<!): Lookbehind\n","    #   → \\S : 공백이 아닌 문자 (non-space)\n","    #   →  공백이 아닌 문자가 바로 앞에 없어야 한다\n","    # (?!\\S) : 바로 뒤가 '공백' 또는 줄의 끝 (단어 경계 뒤)\n","    #   → (?! ): Lookahead\n","    #   → 공백이 아닌 문자가 바로 뒤에 없어야 한다\n","    # → 이 조건은 'e s'처럼 정확히 공백으로 분리된 문자쌍만 매칭\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","\n","    # vocab의 모든 단어에 대해 문자쌍 병합\n","    for word in v_in:\n","        # word = 'n e w e s t '\n","\n","        # p.sub(a, b): b 문자열에서 p에 매칭되는 부분을 a로 바꿔줌\n","        # 'e s' → 'es' 로 바꾸기\n","        # 'n e w e s t ' → 'n e w es t '\n","        # → 정규표현식으로 해당 문자쌍을 붙여서 하나의 토큰으로 만듦\n","        w_out = p.sub(''.join(pair), word)\n","\n","        # 병합된 단어를 새로운 vocab에 저장 (기존 빈도수 유지)\n","        v_out[w_out] = v_in[word]\n","\n","    # 병합된 새 vocab와 병합된 토큰 문자열('es') 반환\n","    return v_out, pair[0] + pair[1]\n","\n","\n","# 병합된 토큰 목록 저장 (최종 subword vocabulary)\n","token_vocab = []\n","\n","# 가장 자주 등장하는 문자쌍 병합\n","for i in range(num_merges):\n","    print(\">> Step {0}\".format(i + 1))\n","\n","    # 현재 vocab에서 모든 문자쌍 빈도수 계산\n","    pairs = get_stats(vocab)\n","    # 가장 자주 등장한 문자쌍\n","    # [참고] max() 함수는 반드시 iterable 하나를 기준으로 동작\n","    #       → 딕셔너리를 iterable로 사용하면 기본 반복 대상은 key\n","    best = max(pairs, key=pairs.get)  # 비교 기준: key(pairs의 value)\n","    # 해당 문자쌍을 병합하여 vocab 업데이트\n","    vocab, merge_tok = merge_vocab(best, vocab)\n","    print(\"다음 문자 쌍을 치환:\", merge_tok)\n","    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n","\n","    # 병합된 토큰 기록\n","    token_vocab.append(merge_tok)\n","\n","# 최종 병합된 토큰 리스트\n","print(\"Merged Vocab:\", token_vocab)"],"metadata":{"id":"-BL0w1jfs44L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"970b8308-6321-450b-9ca4-54092a6b6447","executionInfo":{"status":"ok","timestamp":1749705368652,"user_tz":-540,"elapsed":50,"user":{"displayName":"주리","userId":"05095618937725931730"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":[">> Step 1\n","다음 문자 쌍을 치환: es\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n","\n",">> Step 2\n","다음 문자 쌍을 치환: est\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 3\n","다음 문자 쌍을 치환: lo\n","변환된 Vocab:\n"," {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 4\n","다음 문자 쌍을 치환: low\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 5\n","다음 문자 쌍을 치환: ne\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n","\n","Merged Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"B_6aFdUBkSMD"},"execution_count":null,"outputs":[]}]}